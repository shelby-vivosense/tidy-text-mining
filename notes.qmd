---
title: "notes"
format: html
editor: source
---

# The tidy text format

The example below demonstrates how to converting text to the tidy text format. As an example, we use text from Jane Austen's 6 novels from the `janeaustenr` package:

```{r}

library(janeaustenr)
library(dplyr)
library(tidyr)
library(stringr)

# return tidy dataframe of book text
# with text for each chapter and line number
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(
           text,
           regex('^chapter [\\divxlc]',
                 ignore_case = TRUE)))) %>%
  ungroup()

head(original_books)

```

## Convert to tidy text format

We convert this to a tidy dataset, e.g. one token per row, using the `unnest_tidy()` function. The first argument specifies the new column to create, and the second argument specifies the input column. By default, the function tokenizes for words, but tokenization by characters, sentences, and n-grams is also possible:

```{r}

library(tidytext)

# convert dataframe to tidy (one token per line)
tidy_books <- original_books %>%
  unnest_tokens(word, text)

head(tidy_books)

```

## Remove stop words

To remove stopwords, we use an `anti_join()` with the `stop_words` dataframe, stored by the `tidytext` package:

```{r}

# remove stop words
tidy_books <- tidy_books %>%
  anti_join(stop_words,
            by = 'word')

```

Notice that `stop_words` has stop words from three lexicons; we can use them all together or filter to use only one set:

```{r}

stop_words$lexicon %>% unique()

```

## Count most common words

With the data in tidy format, we can find the most common words in the dataset:

```{r}

tidy_books %>%
  count(word, sort = TRUE)

```

## Visualize most common words

We can also visualize the most common words with `ggplot2`:

```{r}

library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = n, 
             y = word)) +
  geom_col() +
  labs(x = 'Count', 
       y = '',
       subtitle = "The most common words in Jane Austen's novels") +
  theme_minimal()

```

## Comparing word frequencies across texts

For this example, we will compare the frequency of words in Jane Austen's novels with frequencies in two other texts, novels by H.G. Wells, obtained via the `gutenbergr` package:

```{r}

library(gutenbergr)

# download text from novels of HG Wells
hgwells <- gutenberg_download(c(35, 36, 5230, 159))

# convert to tidy format and remove stop words
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# show most common words 
tidy_hgwells %>%
  count(word, sort = TRUE)

```

We'll also gather words from novels of the Brontë sisters using `gutenbergr`:

```{r}

# download text from novels of Bronte sisters
bronte <- gutenberg_download(c(1260, 678, 969, 9182, 767))

# convert to tidy format and remove stop words
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# show most common words 
tidy_bronte %>%
  count(word, sort = TRUE)

```

We calculate the frequency for each word for each set of works, and then compare the frequencies, by binding the dataframes together:

```{r}

frequency <- bind_rows(
  # bind dataframes, adding author column to each
  tidy_bronte %>%
    mutate(author = 'Brontë Sisters'),
  tidy_hgwells %>%
    mutate(author = 'H.G. Wells'),
  tidy_books %>%
    mutate(author = 'Jane Austen')
) %>%
  # this step is bcause UTF-8 encoded texts from Gutenberg
  # have some words with underscores around them
  mutate(word = str_extract(word, "[a-z']+")) %>%
  # tally words for each author
  count(word, author) %>%
  # group by author
  group_by(author) %>%
  # compute frequency of each word
  mutate(proportion = n / sum(n)) %>%
  select(-n)

```

## Visualize comparison of word frequencies

We can visualize Jane Austen novels with words in each of the other groups of novels:

```{r}

library(scales)

# reshape data to reflect the comparisons we want to plot
# (keeping austen proportions in one column)
frequency_reshaped <- frequency %>%
  pivot_wider(names_from = author,
              values_from = proportion) %>%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = 'author', 
               values_to = 'proportion')

ggplot(frequency_reshaped,
       aes(x = proportion,
             y = `Jane Austen`,
             colour = abs(`Jane Austen` - proportion))) +
  geom_abline(colour = 'gray40', lty = 2) +
  geom_jitter(alpha = 0.1,
              size = 2.5,
              width = 0.3, 
              height = 0.3) +
  geom_text(aes(label = word),
            check_overlap = TRUE, 
            vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_colour_gradient(limits = c(0, 0.001),
                        low = 'darkslategray4',
                        high = 'gray75') +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position = 'none') +
  labs(x = '',
       y = 'Jane Austen')

```

## Quantifying similarities in word frequencies

```{r}

cor.test(data = frequency_reshaped[frequency_reshaped$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)

cor.test(data = frequency_reshaped[frequency_reshaped$author == "H.G. Wells",],
         ~ proportion + `Jane Austen`)

```

The results indicate that word frequencies are more correlated between the Austen and Brontë novels than between the Austen and H.G. Wells novels.

# Analyzing word and document frequency: tf-idf

The previous approach to tallying word (term) frequencies, involving anti-joining with a list of stopwords. was not terribly sophisticated. Another approach is to look at a term's inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words not used much in a collection of documents. Together with term frequency (tf), the **tf-idf (which is the two quantities multiplied together) reflects the frequency of a term, adjusted for how rarely it is used**:

$idf(term) = \ln{\frac{n_{documents}}{n_{documents\,containing\,term}}}$

## Zipf's law

We start by examining term frequency and td-idf within Jane Austen's novels.

```{r}

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>%
  group_by(book) %>%
  summarize(total = sum(n))

book_words <- left_join(
  book_words, total_words
) %>%
  rowwise() %>%
  mutate(term_frequency = n/total)

ggplot(book_words, 
       aes(x = term_frequency, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = 'free_y')

```

The distributions in the figure above demonstrate **Zipf's law**, which states that the frequency a word appears is inversely proportional to its rank (rank of the word within the frequency tables created above).

## Calculate tf-idf

Calculating tf-idf attempts to find words that are important (common), but not too common, in a text. The `bind_tf_idf()` function creates columns `tf`, `idf`, and `tf_idf`. We then want to examine words with high tf-idf:

```{r}

book_tf_idf <- book_words %>%
  bind_tf_idf(term = word, document = book, n = n)

book_tf_idf %>%
  select(-total) %>%
  arrange(desc(tf_idf))

```

## Visualize tf-idf

```{r}

library(forcats)

book_tf_idf %>%
  group_by(book) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  ggplot(aes(x = tf_idf, 
             y = fct_reorder(word, tf_idf),
             fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = 'tf-idf', y = NULL) +
  facet_wrap(~book, ncol = 2, scales = 'free')

```

As another example, we will calculate and visualize tf-idf for a set of physics texts from Project Gutenberg:

```{r}

library(gutenbergr)

# download the texts
physics <- gutenberg_download(
  c(37729, 14725, 13476, 30155),
  meta_fields = 'author'
)

# count n for each word in each text
physics_words <- physics %>%
  unnest_tokens(word, text) %>%
  count(author, word, sort = TRUE)

# calculate tf-idf
physics_tfidf <- physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(author = factor(author, levels = c('Galilei, Galileo',
                                            'Huygens, Christiaan',
                                            'Tesla, Nikola',
                                            'Einstein, Albert')))

# show words with highest tf-idf
physics_tfidf %>%
  group_by(author) %>%
  slice_max(tf_idf, n = 2)

# visualize tf-idf
physics_tfidf %>%
  group_by(author) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, tf_idf)) %>%
  ggplot(aes(x = tf_idf,
             y = word,
             fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = 'tf-idf', y = NULL) +
  facet_wrap(~author, ncol = 2, scales = 'free')

```

We can remove some of the less meaningful words:

```{r}

custom_stopwords <- tibble(
  word = c('eq', 'co', 'rc', 'ab', 'ac', 'ak', 
           'bn', 'fig', 'file', 'cg', 'cb', 'cm',
           '_k', '_k_', '_x')
)

# remove custom stop words
physics_words <- anti_join(
  physics_words, custom_stopwords,
  by = 'word'
)

# recalculate tf-idf and visualize
physics_words %>%
  bind_tf_idf(word, author, n) %>%
  mutate(word = str_remove_all(word, '_')) %>%
  group_by(author) %>%
  slice_max(tf_idf, n = 10) %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  mutate(author = factor(author, levels = c('Galilei, Galileo',
                                            'Huygens, Christiaan',
                                            'Tesla, Nikola',
                                            'Einstein, Albert'))) %>%
  ggplot(aes(x = tf_idf,
             y = word,
             fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = 'tf-idf', y = NULL) +
  facet_wrap(~author, ncol = 2, scales = 'free')  

```
